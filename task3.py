# -*- coding: utf-8 -*-
"""Task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eh5zsxgSHZZWZPCtsSHGaj1ED0_KN8mj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split

mnist = fetch_openml('mnist_784', as_frame=False)  #Fetching the MNIST dataset using fetch_openml, specifying the dataset name 'mnist_784'.
# To return the data as a numpy array rather than panda Data frame as_frame = False set.

#Extaracting the feature data(images of handwritten digits from) from the fetched dataset stroring into X variable, similary  extarcting 'target' feature into the y
Bishal,Kharel  = mnist['data'],mnist['target']

#Converting target to integer
# y = y.astype(np.uint8)

"""**Student Name : Bishal Kharel**

**Student_Roll : S00354541**
"""

#Spliting the last two digits from Student_ID
Student_ID = 'S00354541'
last_two_digits = int(Student_ID[-2:]) #Using Slicing method to slice the last two value
print(last_two_digits)

# Importing the necessary function for splitting the dataset
from sklearn.model_selection import train_test_split

# Splitting the dataset into training and testing sets
# test_size=10000: 10,000 samples will be in the testing set
# train_size=60000: 60,000 samples will be in the training set
# random_state: Ensures reproducibility using the last two digits of the student ID
Bishal_train, Bishal_test, Kharel_train, Kharel_test = train_test_split(
    Bishal, Kharel, test_size=10000, train_size=60000, random_state=last_two_digits
)

# Printing the shape of the training and testing sets
# Bishal_train.shape and Kharel_train.shape will give the dimensions of the training features and labels
# Bishal_test.shape and Kharel_test.shape will give the dimensions of the testing features and labels
print("Training set shape:", Bishal_train.shape, Kharel_train.shape)
print("Testing set shape:", Bishal_test.shape, Kharel_test.shape)

# Further splitting the training set into training and validation sets
# test_size=0.2: 20% of the training set will be used for validation
Bishal_train, Bishal_val, Kharel_train, Kharel_val = train_test_split(
    Bishal_train, Kharel_train, test_size=0.2, random_state=last_two_digits
)

# After this split:
# Bishal_train and Kharel_train contain the new training set (80% of the original training set)
# Bishal_val and Kharel_val contain the validation set (20% of the original training set)

"""2. kNN Classifier

a. Set k=10. Use the kNN classifier from scikit-learn, explaining the function parameters and their implications.

b. Evaluate using metrics from the workshop. Discuss why specific metrics were chosen and what they indicate about the model.

c.Experiment with different k values. Discuss any findings and insights regarding the choice of k.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Initializing kNN with k=10
# n_neighbors: Number of neighbors to use (k=10)
# weights: 'uniform' means all points in each neighborhood are weighted equally
# algorithm: 'auto' lets the algorithm choose the best method based on input data
# leaf_size: Size of the leaf in the tree structure (used in 'ball_tree' or 'kd_tree')
k = 10
knn_classifier = KNeighborsClassifier(n_neighbors=k,weights='uniform',
algorithm = 'auto',leaf_size = 30)

# Fitting the classifier on the training data
# The model is trained on Bishal_train features and Kharel_train labels
knn_classifier.fit(Bishal_train, Kharel_train)

# evaluating the model on the validation set:
#Scoring the model using Bishal_val features and Kharel_val labels
sore =knn_classifier.score(Bishal_val, Kharel_val)
print(f'With k ={k} the score is {sore}')

# Predicting the labels for the training data using the trained kNN classifier
Kharel_train_pred = knn_classifier.predict(Bishal_train)

# Comparing the predicted labels with the label '5'
# This will create a boolean array where each element is True if the predicted label is '5' and False otherwise
Kharel_train_pred_5 = (Kharel_train_pred == '5')

# Creating boolean arrays for the training labels
# Kharel_train_5: True if the label is '5', False otherwise
Kharel_train_5 = (Kharel_train == '5')

# Creating boolean arrays for the testing labels
# Kharel_test_5: True if the label is '5', False otherwise
Kharel_test_5 = (Kharel_test == '5')

# Printing the shape of Bishal_train
# Bishal_train.shape gives the dimensions of the training features
print('Bishal_train.shape:', Bishal_train.shape)

# Printing the shape of Kharel_train_5
# Kharel_train_5.shape gives the dimensions of the boolean array indicating labels that are '5'
print('Kharel_train_5.shape:', Kharel_train_5.shape)

# Printing the first ten boolean values indicating whether the training labels are '5'
print(Kharel_train_5[:10])

# Importing the necessary function for SGDClassifier
from sklearn.linear_model import SGDClassifier

# Initializing the SGDClassifier
# max_iter: Maximum number of passes over the training data (epochs)
# tol: Tolerance for stopping criterion
# random_state: Ensures reproducibility using the last two digits of the student ID
sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=last_two_digits)

# Fitting the SGDClassifier on the training data
# Bishal_train: Training features
# Kharel_train_5: Boolean labels indicating if the label is '5'
sgd_clf.fit(Bishal_train, Kharel_train_5)

# Printting a message indicating the model has been trained
print("SGDClassifier has been trained on the training data.")

# Selecting a sample from the dataset (the first digit in this case)
some_digit = Bishal[0]

# Predicting whether this digit is a '5' using the trained SGD classifier
# The prediction will output an array with a single boolean value
# True if the classifier predicts the digit is '5', and False otherwise
prediction = sgd_clf.predict([some_digit])

# Print the prediction result
print(prediction)  # Expected Output: array([ True])

print(Kharel[0])



sgd_clf.predict([Bishal[1]])

print(Kharel[1])

"""**Confusion Matrix:**"""

# Import the necessary function for confusion matrix
from sklearn.metrics import confusion_matrix

# Calculating the confusion matrix using the training data
# Kharel_train_5: Actual labels for the training data indicating whether the digit is '5'
# sgd_clf.predict(Bishal_train): Predicted labels for the training data using the trained SGD classifier
# The confusion matrix shows the counts of true negative, false positive, false negative, and true positive predictions
conf_matrix = confusion_matrix(Kharel_train_5, sgd_clf.predict(Bishal_train))

# Printing the confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

"""**Precision with sklearn**"""

from sklearn.metrics import precision_score, recall_score
precision_score(Kharel_train_5, Kharel_train_pred_5)

cm = confusion_matrix(Kharel_train_5, Kharel_train_pred_5)
cm[1,1]/(cm[0,1]+cm[1,1])

"""**Recall with sklearn**"""

recall_score(Kharel_train_5, Kharel_train_pred_5)

cm[1,1]/(cm[1,0]+cm[1,1])

"""**F1 Score**"""

from sklearn.metrics import f1_score
f1_score(Kharel_train_5, Kharel_train_pred_5)



from sklearn.metrics import classification_report
Kvals = np.arange(1,22,3)

for k in Kvals:
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(Bishal_train, Kharel_train)
    score = knn_classifier.score(Bishal_val, Kharel_val)
    predictions = knn_classifier.predict(Bishal_test)
    print(f'With nearest neighbor {k} accuracy is {score}')
    print(classification_report(Kharel_test,predictions))





from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize the SVM classifier with default parameters
svm_classifier = SVC(C=1.0, kernel='rbf', gamma='scale')

# Train the SVM classifier on the training data
svm_classifier.fit(Bishal_train, Kharel_train)

# Predict the labels for the test set
y_pred = svm_classifier.predict(Bishal_test)

# Calculate accuracy
accuracy = accuracy_score(Kharel_test, y_pred)

# Generate a classification report
class_report = classification_report(Kharel_test, y_pred)

# Generate a confusion matrix
conf_matrix = confusion_matrix(Kharel_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(class_report)
print("Confusion Matrix:")
print(conf_matrix)



!pip install tensorflow

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization

classes = 10
img_rows, img_cols = 100, 100
batch_size = 16

train_data = Bishal_train
valid_data = Kharel_val
test_data = Kharel_test



# Classification Model Designing
model = Sequential()

#First set of conv-relu layer
model.add(Conv2D(64, (3, 3), padding='same',
                 input_shape= (img_rows, img_cols, 3)))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.25))

#Second set of conv-relu layer
model.add(Conv2D(32, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))


model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))


model.add(Dense(classes))
model.add(Activation('softmax'))



import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.utils import to_categorical

# Fetch the MNIST dataset
mnist = fetch_openml('mnist_784', as_frame=False)

# Extract feature data (images) and target labels
Bishal, Kharel = mnist['data'], mnist['target']
Kharel = Kharel.astype(np.uint8)

# Normalize the feature data
Bishal = Bishal / 255.0

# Reshape the data to fit the model
Bishal = Bishal.reshape(-1, 28, 28, 1)

# Convert target labels to categorical format
Kharel = to_categorical(Kharel, 10)

# Splitting the last two digits from Student_ID
Student_ID = 'S00354541'
last_two_digits = int(Student_ID[-2:])

# Split the dataset into training, validation, and testing sets
Bishal_train, Bishal_test, Kharel_train, Kharel_test = train_test_split(
    Bishal, Kharel, test_size=10000, train_size=60000, random_state=last_two_digits
)
Bishal_train, Bishal_val, Kharel_train, Kharel_val = train_test_split(
    Bishal_train, Kharel_train, test_size=0.2, random_state=last_two_digits
)

# Check the shapes to confirm the splits
print('Training data shape:', Bishal_train.shape)
print('Validation data shape:', Bishal_val.shape)
print('Testing data shape:', Bishal_test.shape)

# Classification Model Designing
model = Sequential()

# First set of conv-relu layers
model.add(Conv2D(64, (3, 3), padding='same', input_shape=(28, 28, 1)))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.25))

# Second set of conv-relu layers
model.add(Conv2D(32, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

# Fully connected layers
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(10))
model.add(Activation('softmax'))

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(Bishal_train, Kharel_train, epochs=10, validation_data=(Bishal_val, Kharel_val))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(Bishal_test, Kharel_test, verbose=2)
print(f'Test accuracy: {test_acc}')

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""**5. Critical Analysis:**

a. Compare all the proposed classifiers. Analyze the performance and characteristics of each model
used in this assessment. Create a comprehensive visualization that compares their accuracies,
precision, recall, and F1 scores. Discuss the potential real-world applications of each model based
on their performance metrics and propose improvements or alternative models for those
applications.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score

# Function to calculate and store metrics
def calculate_metrics(y_true, y_pred, classifier_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    return {
        'Classifier': classifier_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }

# kNN Classifier metrics
knn_metrics = []
for k in range(1, 22, 3):
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(Bishal_train, Kharel_train)
    knn_predictions = knn_classifier.predict(Bishal_test)
    metrics = calculate_metrics(Kharel_test, knn_predictions, f'kNN (k={k})')
    knn_metrics.append(metrics)

# SVM Classifier metrics
svm_predictions = svm_classifier.predict(Bishal_test)
svm_metrics = calculate_metrics(Kharel_test, svm_predictions, 'SVM')

# CNN metrics (using history and evaluation results)
cnn_predictions = model.predict(Bishal_test)
cnn_predictions = np.argmax(cnn_predictions, axis=1)
cnn_true_labels = np.argmax(Kharel_test, axis=1)
cnn_metrics = calculate_metrics(cnn_true_labels, cnn_predictions, 'CNN')

# Combine all metrics into a single DataFrame
all_metrics = pd.DataFrame(knn_metrics + [svm_metrics, cnn_metrics])

# Plotting the metrics
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
for i, metric in enumerate(metrics_to_plot):
    sns.barplot(x='Classifier', y=metric, data=all_metrics, ax=axes[i//2, i%2])
    axes[i//2, i%2].set_title(f'Comparison of {metric}')

plt.tight_layout()
plt.show()

# Display the metrics
print(all_metrics)

# Detailed Discussion:
# - Analyze the performance and characteristics of each classifier.
# - Discuss potential real-world applications based on performance metrics.
# - Propose improvements or alternative models for specific applications.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Convert one-hot encoded labels back to 1D array of class labels
Kharel_train_labels = np.argmax(Kharel_train, axis=1)
Kharel_test_labels = np.argmax(Kharel_test, axis=1)
Kharel_val_labels = np.argmax(Kharel_val, axis=1)

# Flatten the 4D image data to 2D for kNN and SVM classifiers
Bishal_train_flat = Bishal_train.reshape(Bishal_train.shape[0], -1)
Bishal_test_flat = Bishal_test.reshape(Bishal_test.shape[0], -1)
Bishal_val_flat = Bishal_val.reshape(Bishal_val.shape[0], -1)

# Function to calculate and store metrics
def calculate_metrics(y_true, y_pred, classifier_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    return {
        'Classifier': classifier_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }

# kNN Classifier metrics
knn_metrics = []
for k in range(1, 22, 3):
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(Bishal_train_flat, Kharel_train_labels)
    knn_predictions = knn_classifier.predict(Bishal_test_flat)
    metrics = calculate_metrics(Kharel_test_labels, knn_predictions, f'kNN (k={k})')
    knn_metrics.append(metrics)

# SVM Classifier metrics
svm_classifier = SVC(C=1.0, kernel='rbf', gamma='scale')
svm_classifier.fit(Bishal_train_flat, Kharel_train_labels)
svm_predictions = svm_classifier.predict(Bishal_test_flat)
svm_metrics = calculate_metrics(Kharel_test_labels, svm_predictions, 'SVM')

# CNN metrics (using history and evaluation results)
cnn_predictions = model.predict(Bishal_test)
cnn_predictions = np.argmax(cnn_predictions, axis=1)
cnn_true_labels = np.argmax(Kharel_test, axis=1)
cnn_metrics = calculate_metrics(cnn_true_labels, cnn_predictions, 'CNN')

# Combine all metrics into a single DataFrame
all_metrics = pd.DataFrame(knn_metrics + [svm_metrics, cnn_metrics])

# Plotting the metrics
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
for i, metric in enumerate(metrics_to_plot):
    sns.barplot(x='Classifier', y=metric, data=all_metrics, ax=axes[i//2, i%2])
    axes[i//2, i%2].set_title(f'Comparison of {metric}')

plt.tight_layout()
plt.show()

# Display the metrics
print(all_metrics)

# Detailed Discussion:
# - Analyze the performance and characteristics of each classifier.
# - Discuss potential real-world applications based on performance metrics.
# - Propose improvements or alternative models for specific applications.



"""**6. Regression Analysis**

Implement Polynomial Regression on the California housing dataset and address the following points:

a. Import the california_housing dataset from the sklearn.datasets

b. Explain the choice of the regression type and its parameters.

c.Evaluate its performance and discuss the results in the context of the chosen model.

d. Analyze the impact of different polynomial degrees on the model's performance.

e. Visualize the relationship between actual vs. predicted prices and discuss any visible trends or
anomalies.
"""

#a. Importing the california_housing dataset from the sklearn.datasets

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
california_housing = fetch_california_housing()

# Split the data into features and target variable
X = california_housing.data
y = california_housing.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to evaluate Polynomial Regression for a given degree
def evaluate_polynomial_regression(degree):
    # Create polynomial features
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    # Fit the model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    # Predict
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)

    # Evaluate the model
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    return train_mse, test_mse, train_r2, test_r2

# Evaluate models with polynomial degrees 1, 2, 3, and 4
degrees = [1, 2, 3, 4]
results = {}
for degree in degrees:
    results[degree] = evaluate_polynomial_regression(degree)

results

for degree in degrees:
    train_mse, test_mse, train_r2, test_r2 = results[degree]
    print(f"Degree: {degree}")
    print(f"Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}")
    print(f"Train R^2: {train_r2:.4f}, Test R^2: {test_r2:.4f}")
    print()

# Visualize actual vs predicted prices for the best degree
best_degree = 2  # Assume degree 2 is the best based on results
poly = PolynomialFeatures(degree=best_degree)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model = LinearRegression()
model.fit(X_train_poly, y_train)
y_test_pred = model.predict(X_test_poly)

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title(f'Actual vs Predicted Prices (Degree {best_degree})')
plt.show()



















